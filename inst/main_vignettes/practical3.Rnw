%\VignetteIndexEntry{practical3}
%\VignetteEngine{Sweave}

<<echo=FALSE>>=
results='hide';echo=FALSE
@
\documentclass[a4paper,justified,openany]{tufte-handout}
<<setup, echo=FALSE, cache=FALSE>>=
dir.create("graphics", showWarnings = FALSE)
library("knitr")
fname = ifelse(echo, "solutions2-", "practical2-")
opts_chunk$set(self.contained=FALSE, tidy = TRUE,
              cache = TRUE, size = "small", message = FALSE,
              fig.path=paste0('knitr_figure/', fname),
               cache.path=paste0('knitr_cache/',fname),
               fig.align='center',
               dev='pdf', fig.width=5, fig.height=5)

knit_hooks$set(par=function(before, options, envir){
    if (before && options$fig.show!='none') {
        par(mar=c(3,3,2,1),cex.lab=.95,cex.axis=.9,
            mgp=c(2,.7,0),tcl=-.01, las=1)
}}, crop=hook_pdfcrop)
opts_knit$set(out.format = "latex")
options(width=56)
@
\usepackage{amsmath}
\usepackage{graphicx}
\setkeys{Gin}{width=\linewidth,totalheight=\textheight,keepaspectratio}
\graphicspath{{graphics/}}
\title{Predictive Analytics: practical 3 \Sexpr{ifelse(echo, "solutions", "")}}
\date{} % if the \date{} command is left out, the current date will be used

\usepackage{booktabs}
\usepackage{units}
\usepackage{fancyvrb}
\fvset{fontsize=\normalsize}
\newcommand{\cc}{\texttt}
\graphicspath{{../graphics/}}
\setcounter{secnumdepth}{2}
\usepackage{microtype}

\begin{document}
\maketitle% this prints the handout title, author, and date

\section*{The \cc{OJ} data set}

The \cc{OJ} data set from the \cc{ISLR} package contains information on which of two brands of orange juice customers purchased\sidenote{The response variable is \cc{Purchase}.} and can be loaded using
<<>>=
data(OJ, package = "ISLR")
@

\noindent After loading the \cc{caret} and \cc{jrPredictive} package
<<message = FALSE>>=
library("caret")
library("jrPredictive")
@

\noindent make an initial examination of the relationships between each of the predictors and the response\sidenote{Use the \cc{plot} function with a model formula or the \cc{pairs} function.}
<<eval = echo, fig.keep="none">>=
par(mfrow = c(4, 5), mar= c(4, .5, .5, .5))
plot(Purchase ~ ., data = OJ)
@

\section*{Initial model building}

\begin{itemize}
\item To begin, create a logistic regression model that takes into consideration the prices of the two brands of orange juice, \cc{PriceCH} and \cc{PriceMM}.\sidenote{Hint: Use the \cc{train} function, with \cc{method = 'glm'}.  Look at the help page for the data set to understand what these
variables represent.}
<<echo=echo, cache=TRUE>>=
m1 = train(Purchase ~ PriceCH + PriceMM,
    data = OJ, method = "glm")
@
  \item What proportion of purchases does this model get right?
<<echo=echo, results=results>>=
mean(predict(m1) != OJ$Purchase)
@
  \item How does this compare to if we used no model?
<<echo=echo, cache=TRUE, results=results>>=
# with no model we essentially predict according to
# proportion of observations in data
probs = table(OJ$Purchase)/nrow(OJ)
preds = sample(levels(OJ$Purchase), prob = probs)
mean(preds != OJ$Purchase)
@
\end{itemize}

\section*{Visualising the boundary}

The \cc{jrPredictive} package contains following code produces a plot of the decision boundary as seen in figure~\ref{fig:purchaseboundary}.
<<figure1,fig.keep="none", echo=2, tidy=FALSE>>=
setnicepar()
boundary_plot(m1,OJ$PriceCH, OJ$PriceMM, OJ$Purchase,
              xlab="Price CH", ylab="Price MM")
@
\noindent Run the boundary code above, and make sure you get a similar plot.

\begin{marginfigure}
<<figure1,echo=FALSE>>=
@
  \caption{Examining the decision boundary for orange juice brand purchases by price.}
  \label{fig:purchaseboundary}
\end{marginfigure}

\begin{itemize}
  \item What happens if we add an interaction term? How does the boundary change?
<<echo=echo, tidy=TRUE>>=
# We now have a curved decision boundary.
# There are two regions of where we would predict MM, bottom left, and a tiny one up in the top right.
@
\item Try adding polynomial terms.
\end{itemize}

\section*{Using all of the predictors}

\begin{itemize}
  \item Fit a logistic regression model using all of the predictors.
<<echo=echo, warning=FALSE>>=
mLM = train(Purchase ~ ., data = OJ, method = "glm")
@
  \item Is there a problem?
<<echo=echo>>=
## YES!
@

\noindent We can view the most recent warning messages by using the \cc{warnings} function
<<eval=FALSE>>=
warnings()
@

<<warning = FALSE, echo = FALSE, cache=TRUE>>=
m_log = train(Purchase ~ ., data = OJ, method = "glm")
@

\noindent This suggests some rank--deficient fit problems,

\item Look at the final model, you should notice that a number of parameters have not been estimated
<<echo=echo, results=results>>=
m_log$finalModel
@

\noindent The help page
<<eval = FALSE, tidy=FALSE>>=
?ISLR::OJ
@
\noindent gives further insight: the \cc{PriceDiff} variable is a linear combination of \cc{SalePriceMM} and \cc{SalePriceCH} so we should remove this. In addition the \cc{StoreID} and \cc{STORE} variable are different encodings of the same information so we should remove one of these too. We also have \cc{DiscCH} and \cc{DiscMM} which are the differences between \cc{PriceCH} and \cc{SalePriceCH} and \cc{PriceMM} and \cc{SalePriceMM} respectively and \cc{ListPriceDiff} is a linear combination of these prices. Removing all of these variables allows the model to be fit and all parameters to be estimated.\sidenote{This is to highlight that we need to understand what we have in our data.}
<<cache=TRUE>>=
OJsub = OJ[,!(colnames(OJ) %in% c("STORE", "SalePriceCH",
           "SalePriceMM","PriceDiff", "ListPriceDiff"))]
OJsub$Store7 = as.numeric(OJsub$Store7) - 1
m_log = train(Purchase ~ ., data = OJsub, method = "glm")
@

\noindent The problem of linear combinations of predictors can be shown with this simple theoretical example. Suppose we have a response $y$ and three predictors $x_1$, $x_2$ and the linear combination $x_3 = (x_1 + x_2)$. On fitting a linear model we try to find estimates of the parameters in the equation
\[
y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \beta_3 (x_1 + x_2).
\]
\noindent However we could just as easily rewrite this as
\begin{align*}
y &= \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \beta_3 (x_1 + x_2) \\
&= \beta_0 + (\beta_1 + \beta_3) x_1 + (\beta_2 + \beta_3) x_2 \\
&= \beta_0 + \beta_1^{\ast} x_1 + \beta_2^{\ast} x_2.
\end{align*}
This leads to a rank--deficient model matrix, essentially we can never find the value of the $\beta_3$ due to the fact we have the linear combination of predictors.

We could achieve the same using the \cc{caret} package function \cc{findLinearCombos}. The function takes a model matrix as an argument. We can create such a matrix using the
\cc{model.matrix} function with our formula object
<<cache=TRUE>>=
remove = findLinearCombos(
               model.matrix(Purchase ~ ., data = OJ))
@
\noindent The output list has a component called \cc{remove} suggesting which variables should be removed to get rid of linear combinations
<<cache=TRUE, tidy=TRUE>>=
(badvar = colnames(OJ)[remove$remove])
OJsub = OJ[, -remove$remove]
@
  \item How accurate is this new model using more predictors?]
<<echo=echo, cache=TRUE, results=results>>=
# the corrected model
remove = findLinearCombos(model.matrix(Purchase~., data = OJ))
(badvar = colnames(OJ)[remove$remove])
OJsub = OJ[,-(remove$remove)]
mLM = train(Purchase~., data = OJsub, method = "glm")
mean(predict(mLM,OJsub) == OJsub$Purchase)
@
  \item What are the values of sensitivity and specificity?
<<echo=echo, results=results>>=
## could use confusionMatrix
(cmLM = confusionMatrix(predict(mLM,OJsub),OJsub$Purchase))

# or
sensitivity(predict(mLM,OJsub),OJsub$Purchase)
specificity(predict(mLM,OJsub),OJsub$Purchase)
@
  \item What does this mean?
<<echo=echo, tidy=TRUE>>=
#The model is fairly good at picking up both positive events, person buys CH, and negative events, MM.
@

\end{itemize}

\section*{ROC curves}

\begin{marginfigure}
<<figure2, eval=TRUE,echo=FALSE, message=FALSE, results="hide">>=
library("pROC")
setnicepar()
curve = roc(response = OJsub$Purchase,
  predictor = predict(m_log, type = "prob")[,"CH"])
## this makes CH the event of interest
plot(curve, legacy.axes = TRUE)
auc(curve)
@
  \caption{An example of a ROC curve for the logistic regression classifier. We can overlay ROC curves by adding the \cc{add = TRUE} argument.}
  \label{fig:roc}
\end{marginfigure}

If we were interested in the area under the ROC curve, we could retrain the model using the \cc{twoClassSummary} function as an argument to a train control object. Alternatively we can
use the \cc{pROC} package

<<message=FALSE>>=
library("pROC")
@

\noindent This also allows us to view the ROC curve, via

<<figure2, echo=3:6, eval=FALSE, message=FALSE,tidy=FALSE>>=
@


\section*{Other classification models}

\begin{itemize}
  \item Try fitting models using the other classification algorithms we have seen so far. To begin with, just have two covariates and use the \cc{boundary\_plot} function to visualise
  the results\marginnote{We have seen LDA, QDA, KNN and logistic regression. Tomorrow we will cover
  support vector machines and neural nets; we can visualise the results in the same way.}
<<echo=echo, cache=TRUE, results=results, message=FALSE>>=
mKNN = train(Purchase~., data = OJsub, method = "knn")
mLDA = train(Purchase~., data = OJsub, method = "lda")
mQDA = train(Purchase~., data = OJsub, method = "qda")
cmKNN = confusionMatrix(predict(mKNN,OJsub),OJsub$Purchase)
cmLDA = confusionMatrix(predict(mLDA,OJsub),OJsub$Purchase)
cmQDA = confusionMatrix(predict(mQDA,OJsub),OJsub$Purchase)
(info = data.frame(Model = c("logistic","knn","lda","qda"),
           Accuracy = c(cmLM$overall["Accuracy"],
               cmKNN$overall["Accuracy"],
               cmLDA$overall["Accuracy"],
               cmQDA$overall["Accuracy"]),
           Sensitivity = c(cmLM$byClass["Sensitivity"],
               cmKNN$byClass["Sensitivity"],
               cmLDA$byClass["Sensitivity"],
               cmQDA$byClass["Sensitivity"]),
           Specificity = c(cmLM$byClass["Specificity"],
               cmKNN$byClass["Specificity"],
               cmLDA$byClass["Specificity"],
               cmQDA$byClass["Specificity"])))
@
  \item How do they compare?
<<echo=echo, tidy=FALSE>>=
#Logistic regression and LDA have highest accuracy, QDA is poorest at classifying events, KNN gives most false positives
@

  \item How does varying the number of nearest neighbours in a KNN affect the model fit?
<<echo=echo, tidy=TRUE, cache=TRUE, results=results>>=
#Accuracy increases at first with knn before then getting worse after a peak value of 9.
(mKNN2 = train(Purchase~., data = OJsub, method = "knn",
    tuneGrid = data.frame(k = 1:30)))
@
\end{itemize}

\noindent The KNN algorithm described in the notes can also be used for regression problems. In this case the predicted response is the mean of the $k$ nearest neighbours.
\begin{itemize}
  \item Try fitting the KNN model for the regression problem in practical 1.
<<warning =FALSE, fig.keep="none", echo=echo, cache=TRUE, results=results, message=FALSE>>=
library("jrPredictive")
data(FuelEconomy, package = "AppliedPredictiveModeling")
regKNN = train(FE~., data = cars2010, method = "knn")
regLM = train(FE~., data = cars2010, method = "lm")
regKNN= validate(regKNN)
regLM = validate(regLM)
mark(regKNN)
mark(regLM)
@

  \item How does this compare to the linear regression models?

<<echo=echo, tidy=TRUE>>=
#The KNN regression model is not as good as the linear model at predicting the test set. It overestimates more at the lower end.
@

\end{itemize}


\section*{Resampling methods}

\begin{itemize}
  \item Fit a KNN regression model to the \cc{cars2010} data set with \cc{FE} as the response.\marginnote{The data set can be loaded \cc{data("FuelEconomy", package = "AppliedPredictiveModeling")}.}
<<echo=echo>>=
mKNN = train(FE ~ ., method = "knn", data = cars2010)
@

  \item Estimate test error using the validation set approach explored at the beginning of the chapter
<<echo=echo>>=
# create a random sample to hold out
i = sample(nrow(cars2010), 100)
# set the train control object
tc = trainControl(method = "cv", number = 1,
    index = list(Fold1 = (1:nrow(cars2010))[-i]))
# fit the model using this train control object
mKNNvs = train(FE~., method = "knn", data = cars2010,
    trControl = tc)
@

  \item Using the same validation set, estimate the performance of the k nearest neighbours algorithm for different values of $k$.
<<echo=echo, cache=TRUE>>=
mKNNvs2 = train(FE~., method = "knn", data = cars2010,
     trControl = tc, tuneGrid = data.frame(k= 2:20))
@
  \item Which model is chosen as the best when using the validation set approach?
<<echo=echo, results=results>>=
## With set.seed(1)
mKNNvs2$bestTune
@

\item Create new \cc{trainControl} objects to specify the use of 5 fold and 10 fold cross validation as well as bootstrapping to estimate test MSE.
<<echo=echo, cache=TRUE>>=
tc5fold = trainControl(method = "cv", number = 5)
tc10fold = trainControl(method = "cv", number = 10)
# use 50 boot strap estimates
tcboot = trainControl(method = "boot", number = 50)
@

  \item Go through the same training procedure attempting to find the best KNN model.
<<echo=echo, results=results, cache=TRUE>>=
mKNNcv5 = train(FE~., data = cars2010, method = "knn",
    trControl = tc5fold, tuneGrid = data.frame(k = 2:20))

mKNNcv10 = train(FE~., data = cars2010, method = "knn",
    trControl = tc10fold, tuneGrid = data.frame(k = 2:20))

mKNNboot = train(FE~., data = cars2010, method = "knn",
    trControl = tcboot, tuneGrid = data.frame(k = 2:20))
mKNNcv5$bestTune
mKNNcv10$bestTune
mKNNboot$bestTune
@

  \item How do the results vary based on the method of estimation?
<<echo=echo, results=results, cache=TRUE,fig.keep="none">>=
#The k-fold cross validation estimates and bootstrap estimates all
#yield the same conclusion, however it is different to when we used
#validation set approach earlier. We could plot the results
# from each on one plot to compare further:
plot(2:20, mKNNboot$results[,2], type = "l", ylab = "RMSE",
     xlab = "k", ylim = c(3,6.5))
lines(2:20, mKNNcv10$results[,2], col = "red")
lines(2:20, mKNNcv5$results[,2], col = "blue")
lines(2:20, mKNNvs2$results[,2], col = "green")
@
  \item Are the conclusions always the same?
<<echo=echo>>=
#no see previous answer
@

\end{itemize}

\noindent If we add the \cc{returnResamp = "all"} argument in the trainControl function we can plot the resampling distributions, see figure~\ref{fig:cvresamp}.

<<fig.keep = "none">>=
tc = trainControl(method = "cv", number = 15,
                  returnResamp = "all")
m = train(FE~., data = cars2010, method = "knn",
          tuneGrid = data.frame(k = 1:15), trControl = tc)
boxplot(RMSE~k, data = m$resample)
@
<<echo = FALSE>>=
pdf("graphics/p3-cvresamp.pdf", width = 6,height = 4)
setnicepar(); mypalette(1)
boxplot(RMSE~k, data = m$resample, xlab = "k", ylab = "RMSE", main = "15 fold cv estimates",
        ylim=c(0, 5), col=3)
sink = dev.off()
system("pdfcrop graphics/p3-cvresamp.pdf && rm graphics/p3-cvresamp.pdf")
@

\begin{figure}[t]
  \centering
  \includegraphics[width = \textwidth]{graphics/p3-cvresamp-crop}
  \caption{$15$ fold cross validation estimates of RMSE in a $K$ nearest neighbours model against number of nearest neighbours.}
  \label{fig:cvresamp}
\end{figure}

We can overlay the information from each method using \cc{add = TRUE}. In addition we could compare the computational cost of each of the methods. The output list from a \cc{train} object contains timing information which can be accessed
<<eval = FALSE>>=
m$time
@

\begin{itemize}
  \item Which method is the most computationally efficient?
<<echo=echo, results=results, cache=TRUE, tidy=TRUE>>=
mKNNvs2$time$everything
mKNNcv5$time$everything
mKNNcv10$time$everything
mKNNboot$time$everything
#The validation set approach was quickest, however we must bear in mind that the conclusion here
#was different to the other cross validation approaches. The two k--fold cross validation estimates of RMSE and the bootstrap
#estimates all agreed with each other lending more weight to their conclusions. Plus we saw in the lectures that validation set
#approach was prone to highly variable estimates meaning we could get a different conclusion using a different hold out set.
#Either of the two k--fold cross validation methods would be preferable here.
@
\end{itemize}

\section*{An example with more than two classes}

The \cc{Glass} data set in the \cc{mlbench} package is a data frame containing examples of the chemical analysis of $7$ different types of glass. The goal is to be able to predict which category glass falls into based on the values of the $9$ predictors.
<<>>=
data(Glass, package = "mlbench")
@

\noindent A logistic regression model is typically not suitable for more than $2$ classes, so try fitting the other models using a training set that consists of 90\% of the available data.\marginnote{The function \cc{createDataPartition} can be used here, see notes for a reminder.}


\section*{Advanced}

\marginnote{This section is intended for users who have a more in depth background to R programming. Attendance to the Programming in R course should be adequate background.}

So far we have only used default functions and metrics to compare the performance of models, however we are not restricted to doing this. For example, training of classification models is typically more difficult when there is an imbalance in the two classes in the training set. Models trained from such data typically have high specificity but poor sensitivity or vice versa. Instead of training to maximise accuracy using data from the training set we could try to maximise according to some other criteria, namely sensitivity and specificity being as close to perfect as possible $(1, 1)$.

To add our function we need to make sure we mirror the structure of those included in caret already.\marginnote{We can view a functions code by typing its name with no brackets.} The following code creates a new function that could be used to summarise a model
<<>>=
fourStats = function (data, lev = NULL, model = NULL) {
  # This code will use the area under the ROC curve and the
  # sensitivity and specificity values from the built in
  # twoClassSummary function
  out = twoClassSummary(data, lev = levels(data$obs),
                        model = NULL)
  # The best possible model has sensitivity of 1 and
  # specifity of 1. How far are we from that value?
  coords = matrix(c(1, 1, out["Spec"], out["Sens"]),
                   ncol = 2,
                   byrow = TRUE)
  # return the disctance measure together with the
  # output from two class summary
  c(Dist = dist(coords)[1], out)
}
@
\noindent we could then use this in the \cc{train} function
<<>>=
data(Sonar, package = "mlbench")
mod = train(Class ~ ., data = Sonar,
              method = "knn",
              # Minimize the distance to the perfect model
              metric = "Dist",
              maximize = FALSE,
              tuneLength = 20,
              trControl =
    trainControl(method = "cv", classProbs = TRUE,
                     summaryFunction = fourStats))
@

\noindent The \cc{plot} function

<<figure3, eval=FALSE>>=
plot(mod)
@

\begin{marginfigure}
\centering
<<figure3, echo=FALSE>>=
@
  \caption{Plot of the distance from a perfect classifier measured by sensitivity and specificity against tuning parameter for a $k$ nearest neighbour model.}
  \label{fig:newsummary}
\end{marginfigure}

\noindent will then show the profile of the resampling estimates of our chosen statistic against the tuning parameters, see figure~\ref{fig:newsummary}.

\begin{itemize}
  \item Have a go at writing a function that will allow a regression model to be chosen by the absolute value of the largest residual and try using it to fit a couple of models.
<<echo=echo, results=results, cache=TRUE, tidy=TRUE, eval=echo, message=FALSE>>=
maxabsres = function(data, lev = NULL, model = NULL){
    m = max(abs(data$obs - data$pred))
    return(c("Max" = m))
}
# Test with pls regression
tccustom = trainControl(method = "cv",
                     summaryFunction = maxabsres)
mPLScustom = train(FE~., data = cars2010,
                   method = "pls",
               tuneGrid = data.frame(ncomp = 1:6),
               trControl = tccustom,
               metric = "Max", maximize = FALSE)
# success
# not to suggest this is a good choice of metric
@

\end{itemize}


\end{document}
